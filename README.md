# ShadowScan - AI-Powered Security Analysis Platform

[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![100% Free AI](https://img.shields.io/badge/AI-100%25%20Free%20(Ollama)-brightgreen.svg)](docs/OLLAMA_SETUP.md)
[![Security Rating](https://img.shields.io/badge/security-A+-green.svg)](https://shadowscan.dev)

> ğŸ‰ **100% GRATUIT** avec Ollama - Aucune API payante requise !
> ğŸŒ **[LIVE DEMO](https://shadowscan.vercel.app)** | ğŸ¤– **[Guide Ollama](docs/OLLAMA_SETUP.md)** | ğŸš€ **[DÃ©ploiement Cloud](DEPLOY.md)**

**ShadowScan** est une plateforme d'analyse de sÃ©curitÃ© alimentÃ©e par l'IA qui utilise **Ollama** (gratuit et local) pour analyser votre code et vos diagrammes d'architecture sans aucun coÃ»t !

---

## âœ¨ Pourquoi ShadowScan ?

- âœ… **100% Gratuit** - Utilise Ollama (modÃ¨les AI locaux)
- âœ… **PrivÃ©** - Vos donnÃ©es ne quittent jamais votre machine
- âœ… **Multi-langages** - Python, JS, TS, Java, Go, Rust, C/C++, PHP, Ruby...
- âœ… **Analyse ComplÃ¨te** - OWASP Top 10 2025, CWE Top 25, secrets, dÃ©pendances
- âœ… **Architecture** - Analyse de diagrammes avec recommandations Zero Trust
- âœ… **ConformitÃ©** - ISO 27001:2022, NIS2, CIS Benchmarks

---

## ğŸš€ DÃ©marrage Rapide (5 minutes)

### PrÃ©requis

- **Docker** et **Docker Compose** installÃ©s
- **8GB RAM minimum** (16GB recommandÃ©)
- **20GB d'espace disque** pour les modÃ¨les AI

### Ã‰tape 1ï¸âƒ£ : Cloner le Projet

```bash
git clone https://github.com/K3E9X/ShadowScan.git
cd ShadowScan
```

### Ã‰tape 2ï¸âƒ£ : DÃ©marrer les Services

```bash
# DÃ©marrer tous les services (Ollama, Backend, Frontend, DB)
docker-compose up -d

# Voir les logs
docker-compose logs -f
```

â³ **Attendez ~2 minutes** que tous les services dÃ©marrent.

### Ã‰tape 3ï¸âƒ£ : TÃ©lÃ©charger les ModÃ¨les AI (IMPORTANT)

```bash
# ModÃ¨le pour l'analyse de code (~4.7GB)
docker-compose exec ollama ollama pull llama3.1:8b

# ModÃ¨le pour l'analyse de diagrammes (~7.4GB)
docker-compose exec ollama ollama pull llava:13b
```

â³ **Temps de tÃ©lÃ©chargement :** 10-20 minutes (selon votre connexion)

ğŸ’¡ **Astuce :** Ces modÃ¨les ne se tÃ©lÃ©chargent qu'une seule fois !

### Ã‰tape 4ï¸âƒ£ : VÃ©rifier que Tout Fonctionne

```bash
# Lister les modÃ¨les tÃ©lÃ©chargÃ©s
docker-compose exec ollama ollama list

# VÃ©rifier les services
docker-compose ps
```

Vous devriez voir tous les services **"Up"** :
```
âœ… shadowscan-ollama    (port 11434)
âœ… shadowscan-backend   (port 8000)
âœ… shadowscan-frontend  (port 3000)
âœ… shadowscan-postgres  (port 5432)
âœ… shadowscan-redis     (port 6379)
```

### Ã‰tape 5ï¸âƒ£ : Ouvrir ShadowScan

ğŸŒ **Frontend** : http://localhost:3000
ğŸ“š **API Docs** : http://localhost:8000/api/docs
ğŸ’š **Health Check** : http://localhost:8000/health

---

## ğŸ§ª Tester ShadowScan

### Test 1 : Analyse de Code

1. Allez sur **http://localhost:3000**
2. Cliquez sur **"Start Analysis"**
3. SÃ©lectionnez **"Code Analysis"**
4. Collez ce code vulnÃ©rable :

```python
import sqlite3

def get_user(user_id):
    conn = sqlite3.connect('database.db')
    cursor = conn.cursor()
    # DANGER: SQL Injection!
    query = "SELECT * FROM users WHERE id = " + user_id
    cursor.execute(query)
    return cursor.fetchone()
```

5. SÃ©lectionnez **"Python"** comme langage
6. Cliquez sur **"Analyze Code"**
7. â³ Attendez **20-40 secondes** (premiÃ¨re analyse plus longue)
8. ğŸ‰ Regardez les vulnÃ©rabilitÃ©s dÃ©tectÃ©es !

**RÃ©sultat attendu :**
- âŒ **SQL Injection** dÃ©tectÃ©e (CWE-89)
- âš ï¸ Niveau de sÃ©vÃ©ritÃ© : **CRITICAL**
- ğŸ’¡ Suggestions de correction avec code sÃ©curisÃ©

### Test 2 : Analyse de Diagramme

1. Allez dans **"Diagram Analysis"**
2. TÃ©lÃ©chargez une image de votre architecture (PNG/JPG/SVG)
3. Cliquez sur **"Analyze Diagram"**
4. â³ Attendez **30-60 secondes**
5. ğŸ‰ Consultez les recommandations Zero Trust !

---

## ğŸ“Š ModÃ¨les AI UtilisÃ©s (Ollama)

| ModÃ¨le | Usage | Taille | Performance CPU |
|--------|-------|--------|-----------------|
| **llama3.1:8b** | Analyse de code | 4.7 GB | 10-60s |
| **llava:13b** | Analyse de diagrammes | 7.4 GB | 20-90s |

### ModÃ¨les Alternatifs (Optionnels)

```bash
# Meilleure prÃ©cision pour le code (plus lent)
docker-compose exec ollama ollama pull codellama:13b

# ModÃ¨le plus puissant (nÃ©cessite 48GB RAM)
docker-compose exec ollama ollama pull mixtral:8x7b

# Vision alternative
docker-compose exec ollama ollama pull bakllava
```

**Changer de modÃ¨le :** Ã‰ditez `docker-compose.yml` :
```yaml
environment:
  - OLLAMA_MODEL_CODE=codellama:13b  # Au lieu de llama3.1:8b
```

---

## âš¡ Optimisation Performance

### Option 1 : GPU NVIDIA (RecommandÃ©)

Si vous avez une carte NVIDIA, dÃ©commentez dans `docker-compose.yml` (lignes 61-67) :

```yaml
deploy:
  resources:
    reservations:
      devices:
        - driver: nvidia
          count: all
          capabilities: [gpu]
```

Puis redÃ©marrez :
```bash
docker-compose down
docker-compose up -d
```

**Performance avec GPU (RTX 3080) :**
- âš¡ Code : **2-15s** (au lieu de 10-60s)
- âš¡ Diagramme : **5-15s** (au lieu de 20-90s)

### Option 2 : Plus de RAM

Dans Docker Desktop â†’ **Settings** â†’ **Resources** :
- **Memory** : Minimum 8GB, RecommandÃ© 16GB
- **CPU** : 4+ cores

---

## ğŸ”§ Commandes Utiles

### Gestion des Services

```bash
# DÃ©marrer
docker-compose up -d

# ArrÃªter
docker-compose down

# RedÃ©marrer
docker-compose restart

# Voir les logs en temps rÃ©el
docker-compose logs -f backend

# Voir l'Ã©tat des services
docker-compose ps
```

### Gestion des ModÃ¨les Ollama

```bash
# Lister les modÃ¨les installÃ©s
docker-compose exec ollama ollama list

# TÃ©lÃ©charger un nouveau modÃ¨le
docker-compose exec ollama ollama pull <model-name>

# Supprimer un modÃ¨le (libÃ©rer de l'espace)
docker-compose exec ollama ollama rm <model-name>

# Tester Ollama directement
docker-compose exec ollama ollama run llama3.1:8b "Analyse ce code Python..."
```

### Nettoyage

```bash
# ArrÃªter et supprimer tout (ATTENTION: supprime les donnÃ©es)
docker-compose down -v

# Supprimer les images Docker
docker-compose down --rmi all

# Rebuild aprÃ¨s changement de code
docker-compose up -d --build
```

---

## ğŸ¯ FonctionnalitÃ©s

### Analyse de Code
- âœ… Support de **14+ langages** (Python, JS, TS, Java, Go, Rust, C/C++, PHP, Ruby, Swift, Kotlin...)
- âœ… DÃ©tection **OWASP Top 10 2025**
- âœ… DÃ©tection **CWE Top 25 2025**
- âœ… DÃ©tection de **secrets** (API keys, mots de passe, tokens)
- âœ… VulnÃ©rabilitÃ©s de **dÃ©pendances**
- âœ… GÃ©nÃ©ration de **code sÃ©curisÃ©**
- âœ… Recommandations de **remÃ©diation**

### Analyse de Diagrammes
- âœ… Support **PNG, JPG, SVG**
- âœ… Identification automatique des **composants**
- âœ… Ã‰valuation de la **posture de sÃ©curitÃ©**
- âœ… Recommandations **Zero Trust**
- âœ… Propositions **Secure-by-Design**
- âœ… Analyse de **conformitÃ©** (ISO 27001, NIS2, CIS)

---

## ğŸ—ï¸ Architecture Technique

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Next.js   â”‚â”€â”€â”€â”€â”€â–¶â”‚   FastAPI    â”‚â”€â”€â”€â”€â”€â–¶â”‚   Ollama    â”‚
â”‚  Frontend   â”‚      â”‚   Backend    â”‚      â”‚  (AI Local) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
                     â”‚             â”‚
                â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”
                â”‚PostgreSQLâ”‚   â”‚ Redis  â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Stack Technique :**
- **Frontend** : Next.js 15, React Server Components, Tailwind CSS
- **Backend** : Python 3.12, FastAPI, SQLAlchemy, Pydantic
- **AI** : Ollama (Llama 3.1, LLaVA)
- **Database** : PostgreSQL 16, Redis 7
- **Infra** : Docker, Kubernetes, Terraform

ğŸ“– [Documentation Architecture ComplÃ¨te](docs/ARCHITECTURE.md)

---

## ğŸ’° Comparaison : Ollama vs APIs Payantes

| CritÃ¨re | Ollama (ShadowScan) | Claude/GPT APIs |
|---------|---------------------|-----------------|
| **CoÃ»t** | **0â‚¬** âœ… | ~$5-15/mois |
| **Setup** | 15 min | 2 min |
| **Vitesse (CPU)** | 30s | 5s |
| **Vitesse (GPU)** | 5s | 5s |
| **ConfidentialitÃ©** | **100% local** âœ… | DonnÃ©es envoyÃ©es |
| **Hors-ligne** | **Oui** âœ… | Non |
| **Limites** | **Aucune** âœ… | Quotas |
| **QualitÃ©** | 85-90% | 95-98% |

**ğŸ’¡ Verdict :** Ollama est **parfait** pour usage personnel, apprentissage et donnÃ©es sensibles !

---

## ğŸš€ DÃ©ploiement en Production

### Option 1 : Vercel + Railway (Gratuit)

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/K3E9X/ShadowScan)
[![Deploy on Railway](https://railway.app/button.svg)](https://railway.app/new/template?template=https://github.com/K3E9X/ShadowScan)

ğŸ“– **[Guide Complet de DÃ©ploiement](DEPLOY.md)**

### Option 2 : Local avec HTTPS

```bash
# Installer Caddy pour HTTPS automatique
docker-compose -f docker-compose.prod.yml up -d
```

---

## ğŸ†˜ ProblÃ¨mes Courants

### âŒ "Cannot connect to Ollama"

```bash
# VÃ©rifier que Ollama est dÃ©marrÃ©
docker-compose ps ollama

# RedÃ©marrer Ollama
docker-compose restart ollama

# Voir les logs
docker-compose logs ollama
```

### âŒ "Model not found"

```bash
# Re-tÃ©lÃ©charger le modÃ¨le
docker-compose exec ollama ollama pull llama3.1:8b
```

### âŒ "Out of memory"

```bash
# Solution 1: Augmenter la RAM Docker (Settings â†’ Resources)
# Solution 2: Utiliser un modÃ¨le plus petit
OLLAMA_MODEL_CODE=llama3.1:8b  # Au lieu de 70b
```

### âŒ "Analysis timeout"

```bash
# C'est normal la premiÃ¨re fois (Ollama charge le modÃ¨le)
# Attendez 1-2 minutes pour la premiÃ¨re analyse
# Les suivantes seront plus rapides (10-30s)
```

### âŒ "Cannot access localhost:3000"

```bash
# VÃ©rifier que les ports ne sont pas utilisÃ©s
lsof -i :3000
lsof -i :8000

# Changer les ports dans docker-compose.yml si besoin
```

---

## ğŸ“š Documentation

- ğŸ¤– **[Guide Ollama (FR)](docs/OLLAMA_SETUP.md)** - Configuration AI locale
- ğŸ—ï¸ **[Architecture](docs/ARCHITECTURE.md)** - Architecture systÃ¨me complÃ¨te
- ğŸš€ **[DÃ©ploiement Cloud](DEPLOY.md)** - Vercel, Railway, Render
- ğŸ¤ **[Contributing](CONTRIBUTING.md)** - Guide de contribution
- ğŸ“– **[API Docs](http://localhost:8000/api/docs)** - Documentation API interactive

---

## ğŸ¤ Contributing

Les contributions sont les bienvenues !

1. Fork le projet
2. CrÃ©ez votre branche (`git checkout -b feature/amazing-feature`)
3. Commit vos changements (`git commit -m 'feat: add amazing feature'`)
4. Push vers la branche (`git push origin feature/amazing-feature`)
5. Ouvrez une Pull Request

ğŸ“– Voir [CONTRIBUTING.md](CONTRIBUTING.md) pour plus de dÃ©tails.

---

## ğŸ”’ SÃ©curitÃ© & ConformitÃ©

### Standards ImplÃ©mentÃ©s

- âœ… **OWASP Top 10 2025**
- âœ… **CWE Top 25 2025**
- âœ… **NIST 800-218 SSDF**
- âœ… **ISO 27001:2022**
- âœ… **NIS2 Directive**
- âœ… **CIS Benchmarks**
- âœ… **GDPR Compliance**

### Signaler une VulnÃ©rabilitÃ©

ğŸ” **Email** : security@shadowscan.dev
âš ï¸ **Ne pas** ouvrir d'issue publique pour les vulnÃ©rabilitÃ©s

---

## ğŸ“„ License

Ce projet est sous licence **MIT** - voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.

---

## ğŸ™ Remerciements

- **Ollama** - Pour les modÃ¨les AI locaux gratuits
- **Meta AI** - Pour Llama 3.1
- **OWASP** - Pour les frameworks de sÃ©curitÃ©
- **MITRE** - Pour la base CWE
- **La communautÃ© open source** â¤ï¸

---

## ğŸ“ Support

- ğŸ’¬ **[GitHub Discussions](https://github.com/K3E9X/ShadowScan/discussions)**
- ğŸ› **[Issues](https://github.com/K3E9X/ShadowScan/issues)**
- ğŸ“§ **Email** : support@shadowscan.dev

---

## â­ Star History

Si ShadowScan vous aide, **donnez une â­ sur GitHub** !

---

**Construit avec â¤ï¸ pour la communautÃ© de sÃ©curitÃ©**

*ShadowScan - SÃ©curisez votre code gratuitement avec l'IA locale*
