# ü§ñ Configuration Ollama pour ShadowScan

## ‚ú® Pourquoi Ollama ?

**Ollama** permet d'ex√©cuter des mod√®les LLM **localement** et **gratuitement** sur votre machine !

### Avantages :
- ‚úÖ **100% Gratuit** - Aucun abonnement API requis
- ‚úÖ **Priv√©** - Vos donn√©es ne quittent jamais votre machine
- ‚úÖ **Rapide** - Pas de latence r√©seau
- ‚úÖ **Hors-ligne** - Fonctionne sans internet (apr√®s t√©l√©chargement)
- ‚úÖ **Open Source** - Mod√®les transparents et v√©rifiables

### Mod√®les Utilis√©s :
- **Llama 3.1 8B** ‚Üí Analyse de code (~4.7GB)
- **LLaVA 13B** ‚Üí Analyse de diagrammes (~7.4GB)
- **Codellama 13B** ‚Üí Alternative pour le code (~7.4GB)

---

## üì¶ Installation Rapide

### Option 1 : Docker (Recommand√©)

**Tout est d√©j√† configur√© !** Les mod√®les seront t√©l√©charg√©s automatiquement.

```bash
# D√©marrer ShadowScan avec Ollama
docker-compose up -d

# Initialiser Ollama et t√©l√©charger les mod√®les
docker-compose exec ollama sh -c "
  ollama pull llama3.1:8b &&
  ollama pull llava:13b
"

# V√©rifier que les mod√®les sont pr√™ts
docker-compose exec ollama ollama list
```

**Temps estim√© :** 15-30 minutes (selon votre connexion)

---

### Option 2 : Installation Locale (Sans Docker)

#### 1. Installer Ollama

**Linux :**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

**macOS :**
```bash
brew install ollama
```

**Windows :**
T√©l√©chargez depuis [ollama.com/download](https://ollama.com/download)

#### 2. D√©marrer Ollama

```bash
ollama serve
```

#### 3. T√©l√©charger les Mod√®les

```bash
# Mod√®le pour l'analyse de code
ollama pull llama3.1:8b

# Mod√®le pour les diagrammes
ollama pull llava:13b

# (Optionnel) Mod√®le alternatif pour le code
ollama pull codellama:13b
```

#### 4. Configurer ShadowScan

Mettez √† jour votre `.env` :

```bash
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL_CODE=llama3.1:8b
OLLAMA_MODEL_VISION=llava:13b
```

---

## ‚öôÔ∏è Configuration Avanc√©e

### Changer de Mod√®le

Vous pouvez utiliser d'autres mod√®les Ollama :

```bash
# Mod√®les disponibles sur ollama.com/library

# Petits mod√®les (rapides, moins pr√©cis)
OLLAMA_MODEL_CODE=llama3.1:8b
OLLAMA_MODEL_CODE=codellama:7b
OLLAMA_MODEL_CODE=mistral:7b

# Grands mod√®les (lents, tr√®s pr√©cis)
OLLAMA_MODEL_CODE=llama3.1:70b
OLLAMA_MODEL_CODE=codellama:34b
OLLAMA_MODEL_CODE=mixtral:8x7b

# Vision models
OLLAMA_MODEL_VISION=llava:13b
OLLAMA_MODEL_VISION=llava:34b
OLLAMA_MODEL_VISION=bakllava
```

### Support GPU

Pour utiliser votre GPU NVIDIA et acc√©l√©rer l'analyse :

1. **Installez NVIDIA Container Toolkit :**
```bash
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update
sudo apt-get install -y nvidia-docker2
sudo systemctl restart docker
```

2. **Le docker-compose.yml est d√©j√† configur√© !**

Le service Ollama d√©tectera automatiquement votre GPU.

### Optimisation M√©moire

Les mod√®les n√©cessitent de la RAM :

| Mod√®le | RAM CPU | VRAM GPU |
|--------|---------|----------|
| llama3.1:8b | 8 GB | 6 GB |
| llava:13b | 16 GB | 10 GB |
| codellama:13b | 16 GB | 10 GB |
| mixtral:8x7b | 48 GB | 40 GB |

**Astuce :** Si vous avez peu de RAM, utilisez des mod√®les plus petits :

```bash
OLLAMA_MODEL_CODE=llama3.1:8b  # Au lieu de 70b
OLLAMA_MODEL_VISION=llava:7b   # Au lieu de 13b
```

---

## üß™ Tester Ollama

### Test Manuel

```bash
# Tester le mod√®le de code
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.1:8b",
  "prompt": "Analyze this Python code for SQL injection: cursor.execute(\"SELECT * FROM users WHERE id = \" + user_id)"
}'

# Tester le mod√®le vision
curl http://localhost:11434/api/generate -d '{
  "model": "llava:13b",
  "prompt": "Describe the security issues in this architecture diagram",
  "images": ["base64_encoded_image_here"]
}'
```

### Test via ShadowScan

1. D√©marrez ShadowScan : `docker-compose up -d`
2. Allez sur http://localhost:3000
3. Analysez du code ou un diagramme
4. V√©rifiez les logs : `docker-compose logs backend`

---

## üöÄ Performance

### Temps d'Analyse Typiques (CPU)

| T√¢che | Mod√®le | Temps |
|-------|--------|-------|
| Code court (< 100 lignes) | llama3.1:8b | 10-30s |
| Code moyen (100-500 lignes) | llama3.1:8b | 30-60s |
| Code long (> 500 lignes) | llama3.1:8b | 1-3min |
| Diagramme simple | llava:13b | 20-40s |
| Diagramme complexe | llava:13b | 40-90s |

### Avec GPU (NVIDIA RTX 3080)

| T√¢che | Mod√®le | Temps |
|-------|--------|-------|
| Code court | llama3.1:8b | 2-5s |
| Code moyen | llama3.1:8b | 5-15s |
| Code long | llama3.1:8b | 15-40s |
| Diagramme | llava:13b | 5-15s |

---

## üîß D√©pannage

### Probl√®me : Ollama ne d√©marre pas

```bash
# V√©rifier les logs
docker-compose logs ollama

# Red√©marrer Ollama
docker-compose restart ollama
```

### Probl√®me : Mod√®les non trouv√©s

```bash
# Lister les mod√®les t√©l√©charg√©s
docker-compose exec ollama ollama list

# Re-t√©l√©charger un mod√®le
docker-compose exec ollama ollama pull llama3.1:8b
```

### Probl√®me : Erreur de m√©moire

```bash
# Utiliser un mod√®le plus petit
OLLAMA_MODEL_CODE=llama3.1:8b  # Au lieu de 70b

# Ou augmenter la RAM Docker
# Dans Docker Desktop ‚Üí Settings ‚Üí Resources ‚Üí Memory
```

### Probl√®me : Analyse trop lente

```bash
# Option 1: Utiliser un mod√®le plus petit
OLLAMA_MODEL_CODE=llama3.1:8b  # Au lieu de 13b/70b

# Option 2: Activer le GPU (voir section Support GPU)

# Option 3: Augmenter les ressources CPU
# Dans docker-compose.yml, pour le service ollama:
deploy:
  resources:
    limits:
      cpus: '8'  # Augmentez selon vos cores disponibles
```

---

## üìä Comparaison : Ollama vs APIs Payantes

| Crit√®re | Ollama | Claude/GPT |
|---------|--------|------------|
| **Co√ªt** | 0‚Ç¨ | ~$0.01-0.10 par analyse |
| **Vitesse (CPU)** | 30s | 5s |
| **Vitesse (GPU)** | 5s | 5s |
| **Confidentialit√©** | ‚úÖ 100% local | ‚ùå Donn√©es envoy√©es |
| **Hors-ligne** | ‚úÖ Oui | ‚ùå Non |
| **Qualit√©** | 85-90% | 95-98% |
| **Setup** | 15 min | 2 min |

**Verdict :** Ollama est parfait pour :
- ‚úÖ Usage personnel/apprentissage
- ‚úÖ Donn√©es sensibles
- ‚úÖ Pas de budget API
- ‚úÖ Utilisation intensive

APIs payantes sont mieux pour :
- ‚úÖ Maximum de pr√©cision
- ‚úÖ Setup rapide
- ‚úÖ Pas de hardware puissant

---

## üÜò Support

Des questions sur Ollama ?

- üìñ [Documentation Ollama](https://github.com/ollama/ollama)
- üí¨ [Discord Ollama](https://discord.gg/ollama)
- üêõ [Issues ShadowScan](https://github.com/K3E9X/ShadowScan/issues)

---

**üéâ Ollama est maintenant configur√© ! Profitez de l'IA gratuite et locale !**
